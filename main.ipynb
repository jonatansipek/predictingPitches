{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Leakage Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS CODE UTILIZES GOOGLE CLOUD, to run this code you need to connect to GOOGLE APIs for using the sheets used in this code\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Function to find timestamps with high correlation to a certain sound\n",
    "def find_offset(episode, jingle, window):\n",
    "    y_within, sr_within = librosa.load(episode, sr=None)\n",
    "    y_find, _ = librosa.load(jingle, sr=sr_within)\n",
    "\n",
    "    # Apply cross-correlation model\n",
    "    c = signal.correlate(y_within, y_find[:sr_within * window], mode='valid', method='fft')\n",
    "\n",
    "    # Find the peaks (timestamps where the correlation is the highest)\n",
    "    peaks, _ = find_peaks(c, prominence=0.1, distance=20)\n",
    "    \n",
    "    # Select top 5 peaks with the highest correlation\n",
    "    top_peaks = np.argsort(c[peaks])[-5:]\n",
    "    peak_indices_sorted = np.sort(peaks[top_peaks])\n",
    "    \n",
    "    # Convert peaks to seconds\n",
    "    offset = [round(peak / sr_within, 2) for peak in peak_indices_sorted]\n",
    "\n",
    "    # Remove peaks that are within the same 30 seconds window\n",
    "    filtered_offset = [offset[0]]\n",
    "    for i in range(1, len(offset)):\n",
    "        if offset[i] - offset[i - 1] > 30:\n",
    "            filtered_offset.append(offset[i])\n",
    "\n",
    "    return filtered_offset\n",
    "\n",
    "# Function to find timestamps with high correlation to a certain sound after a given offset\n",
    "def find_offset_end(episode, jingle, window, offset, duration):\n",
    "    y_within, sr_within = librosa.load(episode, sr=None, offset=offset, duration=duration)\n",
    "    y_find, _ = librosa.load(jingle, sr=sr_within)\n",
    "\n",
    "    # Apply cross-correlation model\n",
    "    c = signal.correlate(y_within, y_find[:sr_within * window], mode='valid', method='fft')\n",
    "\n",
    "    # Find the peaks (timestamps where the correlation is the highest)\n",
    "    peaks, _ = find_peaks(c, prominence=0.1, distance=20)\n",
    "    \n",
    "    # Select top 5 peaks with the highest correlation\n",
    "    top_peaks = np.argsort(c[peaks])[-5:]\n",
    "    peak_indices_sorted = np.sort(peaks[top_peaks])\n",
    "    \n",
    "    # Convert peaks to seconds\n",
    "    offset = [round(peak / sr_within, 2) for peak in peak_indices_sorted]\n",
    "\n",
    "    return offset\n",
    "\n",
    "# Load the Google Sheets where we will store split times\n",
    "worksheet = gc.open('splitTimes').sheet1\n",
    "worksheet2 = gc.open('PitchCount').sheet1\n",
    "\n",
    "# Retrieve expected number of pitches for each episode from the sheet\n",
    "rows = worksheet2.get_all_values()\n",
    "df = pd.DataFrame.from_records(rows, columns=['Episode', 'PitchCount'])\n",
    "\n",
    "# Loop over each episode in the raw video folder\n",
    "raw_videos_path = 'INSERT YOUR GOOGLE PATH HERE'\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    file_path = os.path.join(raw_videos_path, filename)\n",
    "\n",
    "    # Call the offset function to get a list of timestamps where the starting sound occurs\n",
    "    offsets = find_offset(file_path, \"INSERT YOUR GOOGLE PATH HERE\", 10)\n",
    "\n",
    "    # Check if the number of pitches matches the expected count\n",
    "    row = df[df['Episode'] == filename[:-4]]\n",
    "    flag = 0\n",
    "    if not row.empty:\n",
    "        value = row.iloc[0]['PitchCount']\n",
    "        if int(value) != len(offsets):\n",
    "            flag = 1\n",
    "    else:\n",
    "        flag = 2\n",
    "\n",
    "    for index, value in enumerate(offsets):\n",
    "        # Find the first empty row in column A\n",
    "        row = 1\n",
    "        while worksheet.cell(row, 1).value != '':\n",
    "            row += 1\n",
    "\n",
    "        # Fill in timestamps and flag\n",
    "        worksheet.update_cell(row, 1, filename)\n",
    "        worksheet.update_cell(row, 2, value)\n",
    "        worksheet.update_cell(row, 4, flag)\n",
    "        worksheet.update_cell(row, 5, f\"{filename[:-4]}P{index}\")\n",
    "\n",
    "        try:\n",
    "            worksheet.update_cell(row, 3, offsets[index + 1])\n",
    "        except IndexError:\n",
    "            worksheet.update_cell(row, 3, \"Null\")\n",
    "\n",
    "# Update timestamp sheet with correct ending times\n",
    "rows = worksheet.get_all_values()\n",
    "df = pd.DataFrame.from_records(rows, columns=['Episode', 'StartTime', 'EndTime', 'Flag', 'Pitch'])\n",
    "\n",
    "for filename in os.listdir(raw_videos_path):\n",
    "    file_path = os.path.join(raw_videos_path, filename)\n",
    "    video = VideoFileClip(file_path)\n",
    "\n",
    "    # Select the rows in the sheet relevant for this episode\n",
    "    filtered_df = df[df['Episode'] == filename].reset_index(drop=True)\n",
    "\n",
    "    # Loop over the timesplits\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        start_time = row['StartTime']\n",
    "        start = float(start_time) + 15\n",
    "\n",
    "        # Get offset for the ending sound\n",
    "        offsets = find_offset_end(file_path, \"INSERT YOUR GOOGLE PATH HERE\", 10, start, 150)\n",
    "        subclip = video.subclip(start - 4, start + offsets[0])\n",
    "\n",
    "        # Define filepath where we output new file\n",
    "        output_filename_mp4 = f\"{filename[:-4]}P{index}.mp4\"\n",
    "        output_path_mp4 = f\"INSERT YOUR GOOGLE PATH HERE/{output_filename_mp4}\"\n",
    "\n",
    "        # Save subclip\n",
    "        subclip.write_videofile(output_path_mp4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define general paths\n",
    "input_videos_path = \"path/to/input_videos\"\n",
    "output_videos_path = \"path/to/output_videos\"\n",
    "csv_file_path = \"path/to/output_csv/processed_videos_summary.csv\"\n",
    "shark_images_folder = \"path/to/shark_images\"\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(output_videos_path, exist_ok=True)\n",
    "os.makedirs(os.path.dirname(csv_file_path), exist_ok=True)\n",
    "\n",
    "# CSV header updated to include noise ratio and count\n",
    "csv_header = ['ID', 'Shark_ratio', 'Entrepreneur_ratio', 'Noise_ratio', 'Frames', 'Sharks_total', 'Entrepreneurs_total', 'Noise_total', 'Input_video_length', 'Output_video_length']\n",
    "\n",
    "# Function to load shark face encodings from images\n",
    "def load_shark_encodings(shark_images_folder):\n",
    "    shark_encodings = []\n",
    "    for image_name in os.listdir(shark_images_folder):\n",
    "        image_path = os.path.join(shark_images_folder, image_name)\n",
    "        image = face_recognition.load_image_file(image_path)\n",
    "        encodings = face_recognition.face_encodings(image)\n",
    "        if encodings:\n",
    "            shark_encodings.append(encodings[0])\n",
    "    return shark_encodings\n",
    "\n",
    "# Load shark encodings\n",
    "shark_encodings = load_shark_encodings(shark_images_folder)\n",
    "print(f\"Loaded shark encodings: {len(shark_encodings)}\")\n",
    "\n",
    "frame_skip = 2\n",
    "output_fps_option = 15  # Option to switch between 30fps and 15fps for output\n",
    "\n",
    "# Process each video file in the input directory\n",
    "video_files = [f for f in os.listdir(input_videos_path) if f.endswith('.mp4')]\n",
    "for idx, video_file in enumerate(video_files):\n",
    "    video_path = os.path.join(input_videos_path, video_file)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    print(f\"\\nProcessing video {idx+1}/{len(video_files)}: {video_file}\")\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    input_video_length = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    output_fps = fps if output_fps_option == 30 else fps / 2\n",
    "\n",
    "    entrepreneur_video_path = os.path.join(output_videos_path, f'{video_file[:-4]}_entrepreneurs.mp4')\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(entrepreneur_video_path, fourcc, output_fps, (frame_width, frame_height))\n",
    "\n",
    "    total_frames_processed = 0\n",
    "    shark_frames_count = 0\n",
    "    entrepreneur_frames_count = 0\n",
    "    noise_frames_count = 0  # Counter for noise frames\n",
    "\n",
    "    # Adding tqdm for progress bar\n",
    "    for frame_id in tqdm(range(int(input_video_length)), desc=f\"Processing video {idx+1}/{len(video_files)}\"):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_id % frame_skip == 0:\n",
    "            total_frames_processed += 1\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_encodings = face_recognition.face_encodings(frame_rgb)\n",
    "\n",
    "            frame_contains_shark = False\n",
    "\n",
    "            for encoding in frame_encodings:\n",
    "                match_results = face_recognition.compare_faces(shark_encodings, encoding, tolerance=0.4)\n",
    "                if True in match_results:\n",
    "                    frame_contains_shark = True\n",
    "                    shark_frames_count += 1\n",
    "                    break  # Shark detected, skip further processing for this frame\n",
    "\n",
    "            # Write frame to output video if it does not contain a shark\n",
    "            if not frame_contains_shark:\n",
    "                out.write(frame)\n",
    "                if frame_encodings:\n",
    "                    entrepreneur_frames_count += 1\n",
    "                else:\n",
    "                    noise_frames_count += 1  # Count as noise if no faces are detected\n",
    "\n",
    "        frame_id += 1\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Calculating ratios\n",
    "    shark_ratio = shark_frames_count / total_frames_processed if total_frames_processed else 0\n",
    "    entrepreneur_ratio = entrepreneur_frames_count / total_frames_processed if total_frames_processed else 0\n",
    "    noise_ratio = noise_frames_count / total_frames_processed if total_frames_processed else 0\n",
    "    input_video_length_seconds = input_video_length / fps\n",
    "    output_video_length_seconds = (entrepreneur_frames_count + noise_frames_count) / output_fps\n",
    "\n",
    "    # Opening the CSV file in append mode and writing data including noise information\n",
    "    with open(csv_file_path, 'a', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        if os.stat(csv_file_path).st_size == 0:  # Check if the file is empty and write the header\n",
    "            csvwriter.writerow(csv_header)\n",
    "        csvwriter.writerow([video_file.split('.')[0], shark_ratio, entrepreneur_ratio, noise_ratio, total_frames_processed, shark_frames_count, entrepreneur_frames_count, noise_frames_count, input_video_length_seconds, output_video_length_seconds])\n",
    "        \n",
    "    print(f\"\\nFinished processing {video_file}. Total frames processed: {total_frames_processed}. Shark frames: {shark_frames_count}, Entrepreneur frames: {entrepreneur_frames_count}, Noise frames: {noise_frames_count}\")\n",
    "        \n",
    "print(\"\\nPipeline complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from facenet_pytorch import MTCNN\n",
    "from deepface import DeepFace\n",
    "import csv\n",
    "import fnmatch\n",
    "\n",
    "def align_face(image, left_eye_pos, right_eye_pos):\n",
    "    delta_x = right_eye_pos[0] - left_eye_pos[0]\n",
    "    delta_y = right_eye_pos[1] - left_eye_pos[1]\n",
    "    angle = np.arctan(delta_y / delta_x) * 180 / np.pi\n",
    "    eyes_center = ((left_eye_pos[0] + right_eye_pos[0]) // 2, (left_eye_pos[1] + right_eye_pos[1]) // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(eyes_center, angle, scale=1)\n",
    "    aligned_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
    "    return aligned_image, rotation_matrix\n",
    "\n",
    "def round_dict_values(data, decimal_places=8):\n",
    "    return {k: round(v, decimal_places) if isinstance(v, float) else v for k, v in data.items()}\n",
    "\n",
    "def detect_and_analyze_faces(video_path: str, emotion_output_folder: str, race_output_folder: str, no_face_tracker_path: str):\n",
    "    detector = MTCNN(keep_all=True, device='cpu')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Failed to open video: {video_path}\")\n",
    "        return\n",
    "    video_stem = Path(video_path).stem\n",
    "    video_info = video_stem.split('_')[0]\n",
    "\n",
    "    emotions_data = []\n",
    "    races_data = []\n",
    "    no_face_frames = 0\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        boxes, _, landmarks = detector.detect(frame_rgb, landmarks=True)\n",
    "\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            for person_number, (box, landmark) in enumerate(zip(boxes, landmarks), start=1):\n",
    "                if landmark is not None:\n",
    "                    left_eye = landmark[0]\n",
    "                    right_eye = landmark[1]\n",
    "\n",
    "                    aligned_frame, rotation_matrix = align_face(frame, left_eye, right_eye)\n",
    "                    polygon = np.array([[box[0], box[1]], [box[2], box[1]], [box[2], box[3]], [box[0], box[3]]])\n",
    "                    transformed_polygon = cv2.transform(np.array([polygon]), rotation_matrix)[0]\n",
    "                    x1, y1 = np.min(transformed_polygon, axis=0)\n",
    "                    x2, y2 = np.max(transformed_polygon, axis=0)\n",
    "                    x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "                    crop_img = aligned_frame[y1:y2, x1:x2]\n",
    "\n",
    "                    if crop_img.size == 0:\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        results = DeepFace.analyze(crop_img, actions=['age', 'emotion', 'race'], enforce_detection=False)\n",
    "                        result = results[0] if isinstance(results, list) and len(results) > 0 else results\n",
    "\n",
    "                        emotions = round_dict_values(result['emotion'])\n",
    "                        races = round_dict_values(result['race'])\n",
    "                        age = result['age']  # Capture the age data\n",
    "\n",
    "                        frame_id = f\"{video_info}F{frame_count}P{person_number}\"\n",
    "\n",
    "                        emotions_data.append({'ID': frame_id, **emotions})\n",
    "                        races_data.append({'ID': frame_id, 'age': age, **races})\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred when processing frame {frame_count} for person {person_number}: {e}\")\n",
    "        else:\n",
    "            no_face_frames += 1\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    save_data_to_csv(emotions_data, os.path.join(emotion_output_folder, f\"{video_info}.csv\"))\n",
    "    save_data_to_csv(races_data, os.path.join(race_output_folder, f\"{video_info}.csv\"))\n",
    "    save_no_face_data(video_info, no_face_frames, frame_count, no_face_tracker_path)\n",
    "\n",
    "def save_data_to_csv(data_list, file_path):\n",
    "    if not data_list:\n",
    "        return\n",
    "\n",
    "    fieldnames = ['ID'] + sorted({key for d in data_list for key in d if key != 'ID'})\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for data in data_list:\n",
    "            writer.writerow(data)\n",
    "    print(f\"Data saved to {file_path}.\")\n",
    "\n",
    "def save_no_face_data(video_info, no_face_frames, frame_count, no_face_tracker_path):\n",
    "    if not os.path.exists(no_face_tracker_path):\n",
    "        with open(no_face_tracker_path, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['ID', 'NoFaceFrames', 'Total Input Frames', 'Total Output Frames', 'Total Length'])\n",
    "\n",
    "    with open(no_face_tracker_path, 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        output_frames = frame_count - no_face_frames\n",
    "        output_length = output_frames / 15\n",
    "        writer.writerow([video_info, no_face_frames, frame_count, output_frames, output_length])\n",
    "    print(f\"No face data updated for {video_info}.\")\n",
    "\n",
    "def is_video_processed(video_id, no_face_tracker_path):\n",
    "    try:\n",
    "        with open(no_face_tracker_path, mode='r') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                if row['ID'] == video_id:\n",
    "                    return True\n",
    "    except FileNotFoundError:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def process_videos_and_save_data(input_folder: str, emotion_output_folder: str, race_output_folder: str, no_face_tracker_path: str):\n",
    "    os.makedirs(emotion_output_folder, exist_ok=True)\n",
    "    os.makedirs(race_output_folder, exist_ok=True)\n",
    "    for video_file in Path(input_folder).iterdir():\n",
    "        if fnmatch.fnmatch(video_file.name, '*.mp4') or fnmatch.fnmatch(video_file.name, '*.MP4') or fnmatch.fnmatch(video_file.name, '*.mpeg4'):\n",
    "            video_id = video_file.stem.split('_')[0]\n",
    "            if is_video_processed(video_id, no_face_tracker_path):\n",
    "                continue\n",
    "            detect_and_analyze_faces(str(video_file), emotion_output_folder, race_output_folder, no_face_tracker_path)\n",
    "        else:\n",
    "            print(f\"Skipping {video_file.name} due to incompatible file extension.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_input_folder = \"path/to/input_videos\"\n",
    "    emotion_output_folder = \"path/to/emotion_output\"\n",
    "    race_output_folder = \"path/to/race_output\"\n",
    "    no_face_tracker_path = \"path/to/no_face_tracker.csv\"\n",
    "\n",
    "    process_videos_and_save_data(video_input_folder, emotion_output_folder, race_output_folder, no_face_tracker_path)\n",
    "\n",
    "    print(\"All videos processed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaging to get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def calculate_averages(df_sorted):\n",
    "    # Calculate overall and segment averages, excluding the 'ID' and 'FrameNumber' columns\n",
    "    overall_avg = df_sorted.iloc[:, 1:-1].mean().tolist()\n",
    "    num_rows = len(df_sorted) // 3\n",
    "    start_avg = df_sorted.iloc[:num_rows, 1:-1].mean().tolist()\n",
    "    middle_avg = df_sorted.iloc[num_rows:2*num_rows, 1:-1].mean().tolist()\n",
    "    end_avg = df_sorted.iloc[2*num_rows:, 1:-1].mean().tolist()\n",
    "    return overall_avg, start_avg, middle_avg, end_avg\n",
    "\n",
    "def process_files(source_directory, target_directory):\n",
    "    source_path = Path(source_directory)\n",
    "    target_path = Path(target_directory)\n",
    "\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "    results = []\n",
    "\n",
    "    for file_path in source_path.glob('*.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['FrameNumber'] = df['ID'].str.extract('F(\\d+)').astype(int)\n",
    "        df_sorted = df.sort_values(by='FrameNumber', ascending=True)\n",
    "\n",
    "        # Calculate averages\n",
    "        overall_avg, start_avg, middle_avg, end_avg = calculate_averages(df_sorted)\n",
    "\n",
    "        # Construct result row\n",
    "        result_row = [file_path.stem]\n",
    "        for i in range(len(overall_avg)):  # Assume all lists are the same length\n",
    "            result_row.extend([overall_avg[i], start_avg[i], middle_avg[i], end_avg[i]])\n",
    "        results.append(result_row)\n",
    "\n",
    "    if results:\n",
    "        # Create column names for the output file\n",
    "        columns = ['ID']\n",
    "        original_columns = df_sorted.columns[1:-1]  # Exclude 'ID' and 'FrameNumber' columns for naming\n",
    "        segments = ['Overall', 'Start', 'Middle', 'End']\n",
    "        for col in original_columns:\n",
    "            for segment in segments:\n",
    "                columns.append(f\"{col}_{segment}\")\n",
    "\n",
    "        results_df = pd.DataFrame(results, columns=columns)\n",
    "        results_df.to_csv(target_path / 'averages_segmented.csv', index=False)\n",
    "        print(f\"Averages file created at: {target_path / 'averages_segmented.csv'}\")\n",
    "    else:\n",
    "        print(\"No CSV files found in the source directory.\")\n",
    "\n",
    "# Example usage\n",
    "source_directory = \"path/to/source_directory\"\n",
    "target_directory = \"path/to/target_directory\"\n",
    "process_files(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def calculate_average_of_column(df):\n",
    "    # Compute the average of the second column\n",
    "    return df.iloc[:, 1].mean()\n",
    "\n",
    "def process_files(source_directory, target_directory):\n",
    "    source_path = Path(source_directory)\n",
    "    target_path = Path(target_directory)\n",
    "\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each CSV file in the source directory\n",
    "    for file_path in source_path.glob('*.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Calculate the average of the second column\n",
    "        column_avg = calculate_average_of_column(df)\n",
    "\n",
    "        # Append the filename (or ID) and the calculated average to the results list\n",
    "        results.append([file_path.stem, column_avg])\n",
    "\n",
    "    if results:\n",
    "        # Create the DataFrame with column names\n",
    "        results_df = pd.DataFrame(results, columns=['ID', 'Age_average'])\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        results_df.to_csv(target_path / 'column_averages.csv', index=False)\n",
    "        print(f\"Averages file created at: {target_path / 'column_averages.csv'}\")\n",
    "    else:\n",
    "        print(\"No CSV files found in the source directory.\")\n",
    "\n",
    "# Example usage\n",
    "source_directory = \"path/to/source_directory\"\n",
    "target_directory = \"path/to/target_directory\"\n",
    "process_files(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def one_hot_encode_race(df):\n",
    "    # One-hot encode the race columns by setting the dominant race to 1 and others to 0\n",
    "    race_columns = ['asian', 'black', 'indian', 'latino hispanic', 'middle eastern', 'white']\n",
    "    max_values = df[race_columns].max(axis=1)\n",
    "    for col in race_columns:\n",
    "        df[col] = (df[col] == max_values).astype(int)\n",
    "    return df\n",
    "\n",
    "def calculate_ratios(df, race_columns):\n",
    "    # Calculate the ratio of 1s in each race column\n",
    "    ratios = df[race_columns].sum() / len(df)\n",
    "    return ratios.tolist()\n",
    "\n",
    "def process_files(source_directory, target_directory):\n",
    "    source_path = Path(source_directory)\n",
    "    target_path = Path(target_directory)\n",
    "\n",
    "    target_path.mkdir(parents=True, exist_ok=True)\n",
    "    results = []\n",
    "\n",
    "    for file_path in source_path.glob('*.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        df.drop('age', axis=1, inplace=True)\n",
    "        df = one_hot_encode_race(df)\n",
    "\n",
    "        # Calculate ratios for each race\n",
    "        race_columns = ['asian', 'black', 'indian', 'latino hispanic', 'middle eastern', 'white']\n",
    "        ratios = calculate_ratios(df, race_columns)\n",
    "\n",
    "        # Append the filename (or ID) and the calculated ratios to the results list\n",
    "        results.append([file_path.stem] + ratios)\n",
    "\n",
    "    if results:\n",
    "        # Create the DataFrame with column names\n",
    "        columns = ['ID'] + race_columns\n",
    "        results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        results_df.to_csv(target_path / 'race_ratios.csv', index=False)\n",
    "        print(f\"Ratios file created at: {target_path / 'race_ratios.csv'}\")\n",
    "    else:\n",
    "        print(\"No CSV files found in the source directory.\")\n",
    "\n",
    "# Example usage\n",
    "source_directory = \"path/to/source_directory\"\n",
    "target_directory = \"path/to/target_directory\"\n",
    "process_files(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of faces extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def count_frequent_people(df, threshold):\n",
    "    # Extract person identifiers from 'ID' column\n",
    "    person_ids = df['ID'].str.extract(r'P(\\d+)$')[0]\n",
    "    \n",
    "    # Count the frequency of each unique person identifier\n",
    "    person_count = person_ids.value_counts()\n",
    "    \n",
    "    # Filter person identifiers that appear at least as frequently as the threshold\n",
    "    frequent_persons = person_count[person_count >= threshold].index.tolist()\n",
    "    \n",
    "    # Return the count of unique frequent person identifiers\n",
    "    return len(frequent_persons)\n",
    "\n",
    "def process_files(input_directory_path, output_file_path):\n",
    "    input_path = Path(input_directory_path)\n",
    "    people_count_data = []\n",
    "\n",
    "    # Loop over all CSV files in the directory\n",
    "    for filename in input_path.glob('*.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(filename)\n",
    "\n",
    "        # Calculate the threshold for a person to be included (15% of the number of rows)\n",
    "        threshold = len(df) * 0.15\n",
    "\n",
    "        # Count the number of unique people that meet the threshold\n",
    "        number_of_people = count_frequent_people(df, threshold)\n",
    "\n",
    "        # Add the filename and the number of people to the list\n",
    "        people_count_data.append([filename.stem, number_of_people])\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    output_df = pd.DataFrame(people_count_data, columns=['ID', 'Number of people'])\n",
    "\n",
    "    # Write the DataFrame to a new CSV file\n",
    "    output_df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Output file created at: {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_directory_path = \"path/to/input_directory\"\n",
    "output_file_path = \"path/to/output_file.csv\"\n",
    "process_files(input_directory_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbal Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcript extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def process_audio_files(folder_path, target_length_ms, csv_file_path):\n",
    "    audio_data = []\n",
    "\n",
    "    # Loop over each file in the folder\n",
    "    for filename in Path(folder_path).glob('*'):\n",
    "        if filename.suffix in ['.mp3', '.wav']:\n",
    "            full_path = filename\n",
    "\n",
    "            # Load the audio file\n",
    "            audio = AudioSegment.from_file(full_path)\n",
    "\n",
    "            # Add filename and its duration (in seconds) to the list\n",
    "            audio_data.append([filename.name, len(audio) / 1000.0])\n",
    "\n",
    "            # Check if the audio file is longer than the target length\n",
    "            if len(audio) > target_length_ms:\n",
    "                print(f\"Found a file exceeding target length: {filename.name}\")\n",
    "\n",
    "                # Trim the audio to the target length\n",
    "                trimmed_audio = audio[:target_length_ms]\n",
    "\n",
    "                # Replace the original file with the trimmed file\n",
    "                trimmed_audio.export(full_path, format=filename.suffix[1:])\n",
    "\n",
    "    print(\"Processing complete.\")\n",
    "\n",
    "    # Write the audio data to a CSV file\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Filename', 'Duration (seconds)'])\n",
    "        writer.writerows(audio_data)\n",
    "\n",
    "    print(f\"CSV export complete. File saved at: {csv_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"path/to/audio_files\"\n",
    "target_length_ms = 2 * 60 * 1000 + 15 * 1000  # 2 minutes and 15 seconds\n",
    "csv_file_path = \"path/to/output_file.csv\"\n",
    "process_audio_files(folder_path, target_length_ms, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = OpenAI(api_key=\"your_api_key_here\")\n",
    "\n",
    "def transcribe_audio_files(input_folder, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Loop over each audio file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.mp3') or filename.endswith('.wav'):\n",
    "            print(f\"Processing file: {filename}\")\n",
    "\n",
    "            # Get the full file path\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            # Open the audio file\n",
    "            with open(file_path, \"rb\") as audio_file:\n",
    "                # Create the transcription using OpenAI's Whisper model\n",
    "                transcription = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\",\n",
    "                    file=audio_file\n",
    "                )\n",
    "\n",
    "            print(transcription.text)\n",
    "\n",
    "            # Define the output CSV file path\n",
    "            output_path = os.path.join(output_folder, f\"{filename[:-4]}.csv\")\n",
    "\n",
    "            # Write the transcription to a CSV file\n",
    "            with open(output_path, 'w', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Filename\", \"Transcript\"])\n",
    "                writer.writerow([filename[:-4], transcription.text])\n",
    "\n",
    "    print(\"Processing and transcription complete.\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"path/to/input_folder\"\n",
    "output_folder = \"path/to/output_folder\"\n",
    "transcribe_audio_files(input_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Psychological Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure to have these downloaded before running the script\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by converting to lowercase, removing punctuation, \n",
    "    tokenizing, removing stopwords, and lemmatizing.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    return lemmatized_words, len(lemmatized_words)\n",
    "\n",
    "def analyze_transcripts(folder_path, dict_path, output_path):\n",
    "    # Load the dictionary CSV and preprocess the relevant column\n",
    "    dict_df = pd.read_csv(dict_path)\n",
    "    dict_df['preprocessed word 3 (lemmatized)'] = dict_df['preprocessed word 3 (lemmatized)'].str.lower()\n",
    "\n",
    "    categories = ['Sociability', 'Morality', 'Ability', 'Agency']\n",
    "    dataframes_list = []\n",
    "\n",
    "    # Loop through all files in the specified directory\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            text_df = pd.read_csv(file_path)\n",
    "            text_to_check = str(text_df.iloc[0, 1])\n",
    "            preprocessed_words, total_words = preprocess_text(text_to_check)\n",
    "\n",
    "            # Initialize counters\n",
    "            counters = {f'Positive {c}': 0 for c in categories}\n",
    "            counters.update({f'Negative {c}': 0 for c in categories})\n",
    "\n",
    "            # Count matches and directions\n",
    "            for word in preprocessed_words:\n",
    "                matched_rows = dict_df[dict_df['preprocessed word 3 (lemmatized)'] == word]\n",
    "                for _, row in matched_rows.iterrows():\n",
    "                    for category in categories:\n",
    "                        if row[category + ' dictionary'] == 1:\n",
    "                            direction = row[category + ' direction']\n",
    "                            if direction == 1:\n",
    "                                counters[f'Positive {category}'] += 1\n",
    "                            elif direction == -1:\n",
    "                                counters[f'Negative {category}'] += 1\n",
    "\n",
    "            # Calculate ratios\n",
    "            ratios = {k: v / total_words if total_words > 0 else 0 for k, v in counters.items()}\n",
    "            ratios['ID'] = os.path.splitext(filename)[0]\n",
    "            dataframes_list.append(pd.DataFrame([ratios]))\n",
    "\n",
    "    results_df = pd.concat(dataframes_list, ignore_index=True)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"path/to/transcripts\"\n",
    "dict_path = \"path/to/dictionary.csv\"\n",
    "output_path = \"path/to/results_ratios.csv\"\n",
    "analyze_transcripts(folder_path, dict_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mcdonald and Hedonometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "def load_words(file_path):\n",
    "    \"\"\"Load words from a file into a set.\"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        return set(file.read().lower().splitlines())\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by converting to lowercase, removing punctuation, \n",
    "    tokenizing, removing stopwords, and lemmatizing.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return words\n",
    "\n",
    "def analyze_transcripts(category_files_path, transcripts_folder, happiness_dict_path, output_csv_path):\n",
    "    # Load words for each category into a set for faster searching\n",
    "    categories = ['uncertainty', 'positive_finance', 'negative_finance', 'certainty']\n",
    "    category_words = {category: load_words(os.path.join(category_files_path, f'{category}.txt')) for category in categories}\n",
    "\n",
    "    # Load the happiness words and their scores\n",
    "    happiness_dict = pd.read_csv(happiness_dict_path).set_index('word')['happiness_score'].to_dict()\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    # Loop through transcript files and process them\n",
    "    for filename in os.listdir(transcripts_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(transcripts_folder, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                text = file.read()\n",
    "                preprocessed_words = preprocess_text(text)\n",
    "\n",
    "                # Count occurrences and categorize happiness scores\n",
    "                counters = {category: 0 for category in categories}\n",
    "                happiness_positive_count = 0\n",
    "                happiness_negative_count = 0\n",
    "\n",
    "                for word in preprocessed_words:\n",
    "                    for category, words_set in category_words.items():\n",
    "                        if word in words_set:\n",
    "                            counters[category] += 1\n",
    "                    if word in happiness_dict:\n",
    "                        if happiness_dict[word] > 5:\n",
    "                            happiness_positive_count += 1\n",
    "                        elif happiness_dict[word] < 5:\n",
    "                            happiness_negative_count += 1\n",
    "\n",
    "                # Calculate ratios\n",
    "                total_words = len(preprocessed_words)\n",
    "                ratios = {k: v / total_words if total_words > 0 else 0 for k, v in counters.items()}\n",
    "                ratios['Happiness_Positive'] = happiness_positive_count / total_words if total_words > 0 else 0\n",
    "                ratios['Happiness_Negative'] = happiness_negative_count / total_words if total_words > 0 else 0\n",
    "                ratios['ID'] = os.path.splitext(filename)[0]\n",
    "\n",
    "                results_list.append(ratios)\n",
    "\n",
    "    # Convert the list of results to a DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    results_df.to_csv(output_csv_path, index=False)\n",
    "    print(f'Results saved to {output_csv_path}')\n",
    "\n",
    "# Example usage\n",
    "category_files_path = \"path/to/category_files\"\n",
    "transcripts_folder = \"path/to/transcripts\"\n",
    "happiness_dict_path = \"path/to/happiness_dictionary.csv\"\n",
    "output_csv_path = \"path/to/results_ratios.csv\"\n",
    "analyze_transcripts(category_files_path, transcripts_folder, happiness_dict_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WPM, Total Words, Total Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def calculate_words_per_minute(input_folder, durations_file, output_file):\n",
    "    # Read the durations file\n",
    "    durations_df = pd.read_csv(durations_file, delimiter=';')\n",
    "\n",
    "    # Adjust the filenames in the durations dictionary to match the filenames from transcripts\n",
    "    durations_dict = pd.Series(durations_df.iloc[:, 1].values, index=durations_df.iloc[:, 0].str.replace('.wav', '')).to_dict()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Loop through all CSV files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            base_filename = os.path.splitext(filename)[0]\n",
    "            csv_file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "            # Read the CSV file\n",
    "            data = pd.read_csv(csv_file_path)\n",
    "\n",
    "            # Check if the data has the expected 'Transcript' column\n",
    "            if 'Transcript' in data.columns:\n",
    "                words_string = str(data['Transcript'].iloc[0])\n",
    "                word_count = len(words_string.split())\n",
    "                sentence_count = len(re.split(r'[.?!]+', words_string)) - 1\n",
    "\n",
    "                # Retrieve the duration from the dictionary using the base filename\n",
    "                duration_in_seconds = durations_dict.get(base_filename)\n",
    "\n",
    "                # Ensure the duration is valid\n",
    "                if duration_in_seconds and duration_in_seconds > 0:\n",
    "                    total_time_in_minutes = duration_in_seconds / 60\n",
    "                    words_per_minute = word_count / total_time_in_minutes\n",
    "                    results.append([base_filename, words_per_minute, word_count, sentence_count])\n",
    "                else:\n",
    "                    print(f\"Warning: Duration for {base_filename} is zero or missing. Skipping.\")\n",
    "            else:\n",
    "                print(f\"Warning: 'Transcript' column not found in {filename}.\")\n",
    "\n",
    "    # Create a DataFrame for the results\n",
    "    results_df = pd.DataFrame(results, columns=[\"Filename\", \"Words per Minute\", \"Word Count\", \"Sentence Count\"])\n",
    "\n",
    "    # Write the results to the output CSV file\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results written to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"path/to/transcripts\"\n",
    "durations_file = \"path/to/durations.csv\"\n",
    "output_file = \"path/to/results.csv\"\n",
    "calculate_words_per_minute(input_folder, durations_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sure following dependencies are installed\n",
    "#!pip install -q mediapipe\n",
    "#!wget -q https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task\n",
    "#!wget -q -O efficientdet.tflite -q https://storage.googleapis.com/mediapipe-models/object_detector/efficientdet_lite0/int8/1/efficientdet_lite0.tflite\n",
    "#!wget -q https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task\n",
    "#!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task\n",
    "#!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to check if the right hand is higher than the left hand\n",
    "def cross_hand_check(left_hand, right_hand):\n",
    "    return right_hand > left_hand\n",
    "\n",
    "# Function to calculate the bounding box for a face given its landmarks\n",
    "def calculate_face_bbox(face_landmarks, rgb_image):\n",
    "    height, width, _ = rgb_image.shape\n",
    "\n",
    "    x_coordinates = [landmark.x for landmark in face_landmarks]\n",
    "    y_coordinates = [landmark.y for landmark in face_landmarks]\n",
    "\n",
    "    bbox_top_left_x = int(min(x_coordinates) * width)\n",
    "    bbox_top_left_y = int(min(y_coordinates) * height)\n",
    "    bbox_bottom_right_x = int(max(x_coordinates) * width)\n",
    "    bbox_bottom_right_y = int(max(y_coordinates) * height)\n",
    "\n",
    "    bbox_height = bbox_bottom_right_y - bbox_top_left_y\n",
    "    bbox_width = bbox_bottom_right_x - bbox_top_left_x\n",
    "\n",
    "    bbox_top_left_y = max(0, bbox_top_left_y - int(bbox_height * 1.5))\n",
    "    bbox_top_left_x = max(0, bbox_top_left_x - int(bbox_width * 0.25))\n",
    "    bbox_bottom_right_x = min(width, bbox_bottom_right_x + int(bbox_width * 0.25))\n",
    "    bbox_bottom_right_y = min(height, bbox_bottom_right_y + int(bbox_height * 0.5))\n",
    "\n",
    "    return [(bbox_top_left_x, bbox_top_left_y), (bbox_bottom_right_x, bbox_bottom_right_y)]\n",
    "\n",
    "# Function to check if given points are within a bounding box\n",
    "def is_within_bounding_box(points, top_left, bottom_right):\n",
    "    for x, y in points:\n",
    "        top_left_x, top_left_y = top_left\n",
    "        bottom_right_x, bottom_right_y = bottom_right\n",
    "\n",
    "        if top_left_x <= x <= bottom_right_x and bottom_right_y <= y <= top_left_y:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Directory to save cropped images\n",
    "cropped_images_dir = '/content/drive/My Drive/Master thesis/Code/BodyDataCNN/OUTPUT'\n",
    "\n",
    "\n",
    "\n",
    "# Headers for CSV\n",
    "headers = [\n",
    "    \"id\", \"left_wrist_X\", \"left_wrist_Y\", \"right_wrist_X\", \"right_wrist_Y\",\n",
    "    \"crossing_arms\", \"hand_open\", \"touching_face\", \"hand_width\",\n",
    "    \"left_hand_height\", \"right_hand_height\", \"open_pose\"\n",
    "]\n",
    "\n",
    "def create_csv(filename, data_list):\n",
    "    \"\"\"\n",
    "    Creates a CSV file with the specified filename and data.\n",
    "\n",
    "    Args:\n",
    "        filename: The name of the CSV file.\n",
    "        data_list: The list of data to be written to the CSV file.\n",
    "    \"\"\"\n",
    "    csv_file_path = f'/content/drive/My Drive/Master thesis/Code/BodyDataCNN/{filename}.csv'\n",
    "\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "        for data in data_list:\n",
    "            writer.writerow(data)\n",
    "\n",
    "    print(f\"CSV file '{csv_file_path}' created with headers and data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math as m\n",
    "import numpy as np\n",
    "import mediapipe as mp  # Import MediaPipe\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# Define the directory to save cropped images\n",
    "cropped_images_dir = '/content/drive/My Drive/Master thesis/Code/BodyDataCNN/OUTPUT'\n",
    "\n",
    "# Initialize MediaPipe drawing and pose utilities\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True, model_complexity=1, min_detection_confidence=0.5)\n",
    "\n",
    "# Initialize Gesture Recognizer\n",
    "base_options = python.BaseOptions(model_asset_path='gesture_recognizer.task')\n",
    "options = vision.GestureRecognizerOptions(base_options=base_options)\n",
    "recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "# Calculate the angle between two points\n",
    "def find_angle(x1, y1, x2, y2):\n",
    "    theta = m.acos((y2 - y1) * (-y1) / (m.sqrt((x2 - x1)**2 + (y2 - y1)**2) * y1))\n",
    "    degree = int(180 / m.pi) * theta\n",
    "    return degree\n",
    "\n",
    "# Calculate the distance between two points\n",
    "def find_distance(x1, y1, x2, y2):\n",
    "    dist = m.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "    return dist\n",
    "\n",
    "# Map body pose landmarks from an image and save to CSV\n",
    "def map_body_pose(img, filename):\n",
    "    img.flags.writeable = False\n",
    "\n",
    "    # Get joints from pose\n",
    "    results = pose.process(img)\n",
    "\n",
    "    # Get gesture recognition result\n",
    "    mp_image_2 = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "    recognition_result = recognizer.recognize(mp_image_2)\n",
    "\n",
    "    img.flags.writeable = True\n",
    "    image_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    if results.pose_landmarks:\n",
    "        face_box = calculate_face_bbox(results.pose_landmarks.landmark[:11], img)\n",
    "\n",
    "        # Check if one of the hands is touching the face\n",
    "        touching_face = is_within_bounding_box(\n",
    "            [(results.pose_landmarks.landmark[19].x, results.pose_landmarks.landmark[19].y),\n",
    "             (results.pose_landmarks.landmark[20].x, results.pose_landmarks.landmark[20].y)],\n",
    "            face_box[0], face_box[1]\n",
    "        )\n",
    "\n",
    "        # Check if hands are crossed\n",
    "        cross_hand = cross_hand_check(results.pose_landmarks.landmark[15].x, results.pose_landmarks.landmark[16].x)\n",
    "\n",
    "        # Compute hand width\n",
    "        if results.pose_landmarks.landmark[15].visibility > 0.1 and results.pose_landmarks.landmark[16].visibility > 0.1:\n",
    "            hand_distance = results.pose_landmarks.landmark[15].x - results.pose_landmarks.landmark[16].x\n",
    "        else:\n",
    "            hand_distance = -999\n",
    "\n",
    "        # Compute shoulder width\n",
    "        if results.pose_landmarks.landmark[11].visibility > 0.1 and results.pose_landmarks.landmark[12].visibility > 0.1:\n",
    "            shoulder_distance = results.pose_landmarks.landmark[11].x - results.pose_landmarks.landmark[12].x\n",
    "        else:\n",
    "            shoulder_distance = -999\n",
    "\n",
    "        # Compute hand width to shoulder width ratio\n",
    "        handWidth_to_shoulderWidth = hand_distance / shoulder_distance if hand_distance != -999 and shoulder_distance != -999 else -999\n",
    "\n",
    "        # Compute left hand height ratio\n",
    "        if results.pose_landmarks.landmark[23].visibility > 0.1 and results.pose_landmarks.landmark[11].visibility > 0.1:\n",
    "            shoulder_hip_distance_left = results.pose_landmarks.landmark[23].y - results.pose_landmarks.landmark[11].y\n",
    "            if results.pose_landmarks.landmark[15].visibility > 0.1:\n",
    "                left_hand_to_shoulder = results.pose_landmarks.landmark[15].y - results.pose_landmarks.landmark[11].y\n",
    "                left_hand_height_ratio = left_hand_to_shoulder / shoulder_hip_distance_left\n",
    "            else:\n",
    "                left_hand_height_ratio = -999\n",
    "        else:\n",
    "            left_hand_height_ratio = -999\n",
    "\n",
    "        # Compute right hand height ratio\n",
    "        if results.pose_landmarks.landmark[24].visibility > 0.1 and results.pose_landmarks.landmark[12].visibility > 0.1:\n",
    "            shoulder_hip_distance_right = results.pose_landmarks.landmark[24].y - results.pose_landmarks.landmark[12].y\n",
    "            if results.pose_landmarks.landmark[16].visibility > 0.1:\n",
    "                right_hand_to_shoulder = results.pose_landmarks.landmark[16].y - results.pose_landmarks.landmark[12].y\n",
    "                right_hand_height_ratio = right_hand_to_shoulder / shoulder_hip_distance_right\n",
    "            else:\n",
    "                right_hand_height_ratio = -999\n",
    "        else:\n",
    "            right_hand_height_ratio = -999\n",
    "\n",
    "        # Compute hand gesture\n",
    "        if recognition_result.hand_landmarks:\n",
    "            top_gesture = recognition_result.gestures[0][0].category_name\n",
    "        else:\n",
    "            top_gesture = 'none'\n",
    "\n",
    "        # Check if the body language is open or not\n",
    "        if handWidth_to_shoulderWidth != -999 and left_hand_height_ratio != -999 and right_hand_height_ratio != -999:\n",
    "            open_pose = 1 if handWidth_to_shoulderWidth > 1 and left_hand_height_ratio < 1 and right_hand_height_ratio < 1 else 0\n",
    "        else:\n",
    "            open_pose = -999\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        final_frame = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Save the processed image to the output directory\n",
    "        # output_path = os.path.join(cropped_images_dir, filename)\n",
    "        # cv2.imwrite(output_path, image_bgr)\n",
    "        # print(f\"Processed and saved: {output_path}\")\n",
    "\n",
    "        # Save to CSV\n",
    "        row_to_add = {\n",
    "            \"id\": filename,\n",
    "            \"left_wrist_X\": results.pose_landmarks.landmark[15].x,\n",
    "            \"left_wrist_Y\": results.pose_landmarks.landmark[15].y,\n",
    "            \"right_wrist_X\": results.pose_landmarks.landmark[16].x,\n",
    "            \"right_wrist_Y\": results.pose_landmarks.landmark[16].y,\n",
    "            \"crossing_arms\": cross_hand,\n",
    "            \"hand_open\": top_gesture,\n",
    "            \"touching_face\": touching_face,\n",
    "            \"hand_width\": handWidth_to_shoulderWidth,\n",
    "            \"left_hand_height\": left_hand_height_ratio,\n",
    "            \"right_hand_height\": right_hand_height_ratio,\n",
    "            \"open_pose\": open_pose\n",
    "        }\n",
    "        return row_to_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Define the directory to save cropped images\n",
    "save_dir = \"/content/drive/My Drive/Master thesis/Code/BodyDataCNN/OUTPUT\"\n",
    "\n",
    "# Initialize the YOLO model\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# Load class list from coco.txt\n",
    "with open(\"/content/drive/My Drive/Master thesis/Code/BodyExtraction/coco/coco.txt\", \"r\") as my_file:\n",
    "    class_list = my_file.read().split(\"\\n\")\n",
    "\n",
    "# Function to save the cropped image\n",
    "def save_cropped_image(img, save_dir, filename):\n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    img_file_name = os.path.join(save_dir, filename)\n",
    "    cv2.imwrite(img_file_name, img)\n",
    "\n",
    "# Function to process image using YOLO model\n",
    "def process_image_yolo(img, file_name, frame_num, scale_factor=0.5):\n",
    "    # Resize the frame for faster processing\n",
    "    frame = cv2.resize(img, (0, 0), fx=scale_factor, fy=scale_factor)\n",
    "    results = model.predict(frame, verbose=False)\n",
    "\n",
    "    boxes = pd.DataFrame(results[0].boxes.data).astype(\"float\")\n",
    "\n",
    "    face_list = []\n",
    "    crop_list = []\n",
    "\n",
    "    for index, row in boxes.iterrows():\n",
    "        x1, y1, x2, y2, conf, d = row\n",
    "        c = class_list[int(d)]\n",
    "        confidence_threshold = 0.8\n",
    "        if 'person' in c and conf >= confidence_threshold:\n",
    "            # Expand the bounding box slightly\n",
    "            expand_by = 0.1\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            x1_new = max(int(x1 - expand_by * width), 0)\n",
    "            y1_new = max(int(y1 - expand_by * height), 0)\n",
    "            x2_new = min(int(x2 + expand_by * width), frame.shape[1])\n",
    "            y2_new = min(int(y2 + expand_by * height), frame.shape[0])\n",
    "\n",
    "            crop = frame[y1_new:y2_new, x1_new:x2_new]\n",
    "            crop_list.append(crop)\n",
    "\n",
    "            # Convert crop to grayscale and detect faces\n",
    "            gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "            if faces is not ():\n",
    "                face_list.append(faces[0])\n",
    "\n",
    "    # Compute face areas\n",
    "    face_areas = [(w * h, (x, y, w, h)) for (x, y, w, h) in face_list]\n",
    "\n",
    "    # Function to find the index of the maximum face area\n",
    "    def find_max_first_value_index(data):\n",
    "        max_value = -float('inf')\n",
    "        max_index = -1\n",
    "        for index, (first_value, _) in enumerate(data):\n",
    "            if first_value > max_value:\n",
    "                max_value = first_value\n",
    "                max_index = index\n",
    "        return max_index\n",
    "\n",
    "    index = find_max_first_value_index(face_areas)\n",
    "    row_to_add = None\n",
    "\n",
    "    # Select the YOLO crop\n",
    "    if crop_list:\n",
    "        if index < len(crop_list):\n",
    "            frame_rgb = cv2.cvtColor(crop_list[index], cv2.COLOR_BGR2RGB)\n",
    "            row_to_add = map_body_pose(frame_rgb, file_name + \"_F\" + str(frame_num) + \".png\")\n",
    "        else:\n",
    "            frame_rgb = cv2.cvtColor(crop_list[0], cv2.COLOR_BGR2RGB)\n",
    "            row_to_add = map_body_pose(frame_rgb, file_name + \"_F\" + str(frame_num) + \".png\")\n",
    "\n",
    "    return row_to_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "# Initialize an empty DataFrame to hold the data\n",
    "df = pd.DataFrame(columns=[\n",
    "    \"id\", \"left_wrist_X\", \"left_wrist_Y\", \"right_wrist_X\", \"right_wrist_Y\",\n",
    "    \"crossing_arms\", \"hand_open\", \"touching_face\", \"hand_width\",\n",
    "    \"left_hand_height\", \"right_hand_height\"\n",
    "])\n",
    "\n",
    "# Path to the input frames directory\n",
    "input_dir = '/content/drive/My Drive/Master thesis/Code/face_and_body_extraction/pitches_without_sharks'\n",
    "\n",
    "# Process each file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    # Read the CSV file to check for blocklisted files\n",
    "    blocklist_path = '/content/drive/My Drive/Master thesis/Code/BodyDataCNN/blocklist.csv'\n",
    "    blocklist_df = pd.read_csv(blocklist_path, header=None)\n",
    "    existing_filenames = blocklist_df.iloc[:, 0].tolist()  # Assuming filenames are in the first column\n",
    "\n",
    "    # Skip the file if it is in the blocklist\n",
    "    if filename in existing_filenames:\n",
    "        continue\n",
    "\n",
    "    video_path = os.path.join(input_dir, filename)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Extract pitch name from the filename\n",
    "    position_of_f = filename.find('_')\n",
    "    pitch_name = filename[:position_of_f]\n",
    "\n",
    "    # Initialize list to hold data for the current video\n",
    "    data_array = []\n",
    "\n",
    "    frame_index = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit loop if no frames are left\n",
    "\n",
    "        # Process the frame with YOLO and map body pose\n",
    "        row_to_add = process_image_yolo(img=frame, file_name=pitch_name, frame_num=frame_index)\n",
    "\n",
    "        if row_to_add is not None:\n",
    "            data_array.append(row_to_add)\n",
    "\n",
    "        frame_index += 1  # Increment the frame index\n",
    "\n",
    "    cap.release()  # Release the video capture object\n",
    "\n",
    "    # Create a new CSV file for the current video data\n",
    "    create_csv(pitch_name, data_array)\n",
    "\n",
    "    # Append the processed filename to the blocklist\n",
    "    with open(blocklist_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([filename])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocal Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import parselmouth\n",
    "from parselmouth.praat import call  # pylint: disable=no-name-in-module, import-error\n",
    "from config import Config\n",
    "import glob\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "def extract_features(filenames):\n",
    "    \"\"\"\n",
    "    Extracts audio features from a list of files and saves the results to CSV files.\n",
    "\n",
    "    Args:\n",
    "        filenames (list): List of file paths to process.\n",
    "\n",
    "    Returns:\n",
    "        output_df (DataFrame): DataFrame containing extracted features.\n",
    "        ipu_df (DataFrame): DataFrame containing IPU (inter-pausal unit) information.\n",
    "    \"\"\"\n",
    "    output_df = pd.DataFrame(columns=[\n",
    "        'filename', 'Min Pitch', 'Max Pitch', 'Mean Pitch', 'Sd Pitch',\n",
    "        'Min Intensity', 'Max Intensity', 'Mean Intensity', 'Sd Intensity',\n",
    "        'Jitter', 'Shimmer', 'HNR', 'Energy'\n",
    "    ])\n",
    "    ipu_df = pd.DataFrame(columns=['filename', 'start_time', 'end_time'])\n",
    "\n",
    "    for count, fn in enumerate(filenames, 1):\n",
    "        if count % 100 == 0:\n",
    "            print('Completed:', count)\n",
    "\n",
    "        try:\n",
    "            sound = parselmouth.Sound(fn)\n",
    "            pitch = call(sound, 'To Pitch', 0.0, 75.0, 600.0)\n",
    "            intensity = call(sound, 'To Intensity', 75.0, 0.0, True)\n",
    "            point_process = call(sound, 'To PointProcess (periodic, cc)', 75.0, 600.0)\n",
    "            harmonicity = call(sound, 'To Harmonicity (cc)', 0.01, 75, 0.1, 1.0)\n",
    "            silence = call(sound, \"To TextGrid (silences)\", 100, 0.0, -25.0, 0.05, 0.1, \"silent\", \"sounding\")\n",
    "            \n",
    "            num_intervals = call(silence, \"Get number of intervals\", 1)\n",
    "            start_times = [\n",
    "                call(silence, \"Get start time of interval\", 1, i)\n",
    "                for i in range(1, num_intervals + 1)\n",
    "                if call(silence, \"Get label of interval\", 1, i) == \"sounding\"\n",
    "            ]\n",
    "            end_times = [\n",
    "                call(silence, \"Get end time of interval\", 1, i)\n",
    "                for i in range(1, num_intervals + 1)\n",
    "                if call(silence, \"Get label of interval\", 1, i) == \"sounding\"\n",
    "            ]\n",
    "\n",
    "            ipu_df = pd.concat([ipu_df, pd.DataFrame([[os.path.basename(fn), start_times, end_times]], columns=ipu_df.columns)], ignore_index=True)\n",
    "\n",
    "            features = {\n",
    "                'min_pitches': [], 'max_pitches': [], 'mean_pitches': [], 'sd_pitches': [],\n",
    "                'min_intensities': [], 'max_intensities': [], 'mean_intensities': [], 'sd_intensities': [],\n",
    "                'jitter_list': [], 'shimmer_list': [], 'hnr_list': [], 'energy_list': []\n",
    "            }\n",
    "\n",
    "            for t1, t2 in zip(start_times, end_times):\n",
    "                # Extract pitch features\n",
    "                features['min_pitches'].append(call(pitch, 'Get minimum', t1, t2, 'Hertz', 'Parabolic'))\n",
    "                features['max_pitches'].append(call(pitch, 'Get maximum', t1, t2, 'Hertz', 'Parabolic'))\n",
    "                features['mean_pitches'].append(call(pitch, 'Get mean', t1, t2, 'Hertz'))\n",
    "                features['sd_pitches'].append(call(pitch, 'Get standard deviation', t1, t2, 'Hertz'))\n",
    "\n",
    "                # Extract intensity features\n",
    "                features['min_intensities'].append(call(intensity, 'Get minimum', t1, t2, 'Parabolic'))\n",
    "                features['max_intensities'].append(call(intensity, 'Get maximum', t1, t2, 'Parabolic'))\n",
    "                features['mean_intensities'].append(call(intensity, 'Get mean', t1, t2, 'energy'))\n",
    "                features['sd_intensities'].append(call(intensity, 'Get standard deviation', t1, t2))\n",
    "\n",
    "                # Extract jitter and shimmer\n",
    "                features['jitter_list'].append(call(point_process, 'Get jitter (local)', t1, t2, 0.0001, 0.02, 1.3))\n",
    "                features['shimmer_list'].append(call([sound, point_process], 'Get shimmer (local)', t1, t2, 0.0001, 0.02, 1.3, 1.6))\n",
    "\n",
    "                # Extract HNR\n",
    "                features['hnr_list'].append(call(harmonicity, \"Get mean\", t1, t2))\n",
    "\n",
    "                # Extract energy\n",
    "                features['energy_list'].append(call(sound, 'Get energy', t1, t2))\n",
    "\n",
    "            for key in features:\n",
    "                features[key] = np.array(features[key])\n",
    "                features[key] = features[key][~np.isnan(features[key])]\n",
    "\n",
    "            # Calculate mean values of features\n",
    "            feature_means = {key: features[key].mean() if len(features[key]) > 0 else np.nan for key in features}\n",
    "\n",
    "            new_row = pd.DataFrame([[\n",
    "                os.path.basename(fn), feature_means['min_pitches'], feature_means['max_pitches'],\n",
    "                feature_means['mean_pitches'], feature_means['sd_pitches'], feature_means['min_intensities'],\n",
    "                feature_means['max_intensities'], feature_means['mean_intensities'], feature_means['sd_intensities'],\n",
    "                feature_means['jitter_list'], feature_means['shimmer_list'], feature_means['hnr_list'],\n",
    "                feature_means['energy_list']\n",
    "            ]], columns=output_df.columns)\n",
    "\n",
    "            output_df = pd.concat([output_df, new_row], ignore_index=True)\n",
    "\n",
    "        except:\n",
    "            print(sys.exc_info())\n",
    "            continue\n",
    "\n",
    "    # Save the results to CSV files\n",
    "    output_df.to_csv(\"/content/drive/MyDrive/Master thesis/Code/praat5.csv\", index=None, header=True)\n",
    "    ipu_df.to_csv(\"/content/drive/MyDrive/Master thesis/Code/ipu5.csv\", index=None, header=True)\n",
    "\n",
    "    return output_df, ipu_df\n",
    "\n",
    "# Example usage:\n",
    "# myargs = glob.glob(\"drive/My Drive/align_speech/*.wav\")\n",
    "# df, ipu_df = extract_features(myargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive analysis plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data from the specified Excel file and sheet\n",
    "file_path = 'path/to/Data.xlsx'\n",
    "sheet_name = 'Cleaned'\n",
    "\n",
    "# Read the data into a pandas DataFrame\n",
    "data = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "# Calculate summary statistics\n",
    "summary_stats = data.describe()\n",
    "\n",
    "# Plot settings\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Plot 1: Age\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=data, x='Age', fill=True, color='skyblue', edgecolor='steelblue')\n",
    "plt.axvline(data['Age'].mean(), color='red', linestyle='--', linewidth=1)\n",
    "plt.xlabel('Age of Entrepreneur')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Age Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Valuation Requested\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=data, x='Valuation Requested', fill=True, color='skyblue', edgecolor='steelblue')\n",
    "plt.axvline(data['Valuation Requested'].mean(), color='red', linestyle='--', linewidth=1)\n",
    "plt.xlabel('Valuation Requested')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Valuation Requested Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: Original Ask Amount\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(data=data, x='Original Ask Amount', fill=True, color='skyblue', edgecolor='steelblue')\n",
    "plt.axvline(data['Original Ask Amount'].mean(), color='red', linestyle='--', linewidth=1)\n",
    "plt.xlabel('Original Ask Amount')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Original Ask Amount Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: Ethnicity\n",
    "ethnicity_columns = ['Asian', 'black', 'indian', 'latino hispanic', 'middle eastern', 'white']\n",
    "data['Ethnicity'] = data[ethnicity_columns].idxmax(axis=1)\n",
    "ethnicity_deal_counts = data.groupby(['Ethnicity', 'DEAL']).size().unstack().fillna(0)\n",
    "ethnicity_deal_counts['Total'] = ethnicity_deal_counts.sum(axis=1)\n",
    "ethnicity_deal_counts = ethnicity_deal_counts.sort_values(by='Total', ascending=False).drop(columns='Total')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ethnicity_deal_counts.plot(kind='barh', stacked=True, color=['skyblue', 'steelblue'])\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Ethnicity')\n",
    "plt.title('Ethnicity Deal Counts')\n",
    "plt.show()\n",
    "\n",
    "# Plot 5: Gender Distribution\n",
    "def determine_gender(row):\n",
    "    if row['Male'] == 1 and row['Female'] == 0:\n",
    "        return 'Male'\n",
    "    elif row['Male'] == 0 and row['Female'] == 1:\n",
    "        return 'Female'\n",
    "    else:\n",
    "        return 'Mixed Gender'\n",
    "\n",
    "data['Gender Distribution'] = data.apply(determine_gender, axis=1)\n",
    "gender_deal_counts = data.groupby(['Gender Distribution', 'DEAL']).size().unstack().fillna(0)\n",
    "gender_deal_counts['Total'] = gender_deal_counts.sum(axis=1)\n",
    "gender_deal_counts = gender_deal_counts.sort_values(by='Total', ascending=False).drop(columns='Total')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "gender_deal_counts.plot(kind='barh', stacked=True, color=['skyblue', 'steelblue'])\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Gender Distribution')\n",
    "plt.title('Gender Distribution Deal Counts')\n",
    "plt.show()\n",
    "\n",
    "# Plot 6: Industry\n",
    "industry_mapping = {\n",
    "    1: 'Lifestyle/Home',\n",
    "    2: 'Electronics',\n",
    "    3: 'Food and Beverage',\n",
    "    4: 'Children/Education',\n",
    "    5: 'Fashion/Beauty',\n",
    "    6: 'Fitness/Sports/Outdoors',\n",
    "    7: 'Uncertain/Other',\n",
    "    8: 'Software/Tech',\n",
    "    9: 'Health/Wellness',\n",
    "    10: 'Automotive',\n",
    "    11: 'Pet Products',\n",
    "    12: 'Media/Entertainment',\n",
    "    13: 'Business Services',\n",
    "    14: 'Liquor/Alcohol',\n",
    "    15: 'Travel',\n",
    "    16: 'Green/CleanTech'\n",
    "}\n",
    "\n",
    "data['Industry'] = data['Industry'].map(industry_mapping)\n",
    "industry_deal_counts = data.groupby(['Industry', 'DEAL']).size().unstack().fillna(0)\n",
    "industry_deal_counts['Total'] = industry_deal_counts.sum(axis=1)\n",
    "industry_deal_counts = industry_deal_counts.sort_values(by='Total', ascending=False).drop(columns='Total')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "industry_deal_counts.plot(kind='barh', stacked=True, color=['skyblue', 'steelblue'])\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Industry')\n",
    "plt.title('Industry Deal Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the preprocessed data from the Excel file\n",
    "file_path = '/content/drive/My Drive/Master thesis/Code/FINAL_DATA_FOR_THESIS/VIF.xlsx'\n",
    "sheet_name = 'VIF Data'\n",
    "\n",
    "# Load the data, ensuring all columns are float except \"ID\" and \"Pitchers Gender\"\n",
    "df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "\n",
    "\n",
    "\n",
    "#FEATURE ENGINEERING\n",
    "\n",
    "\n",
    "#Scale all features\n",
    "features_to_center = df.columns.drop([\"Deal\", 'ID', 'Male', 'Female', 'Ethnicity Asian', 'Ethnicity Black', 'Ethnicity Indian', 'Ethnicity Latino Hispanic', 'Ethnicity Middle Eastern', 'Ethnicity White', 'Industry', '# Of Entrepreneurs'])\n",
    "scaler = StandardScaler(with_std=False)\n",
    "df_centered = pd.DataFrame(scaler.fit_transform(df[features_to_center]), columns=(features_to_center))\n",
    "df_centered = pd.concat([df_centered, df[[\"Deal\", 'ID', 'Male', 'Female', 'Ethnicity Asian', 'Ethnicity Black', 'Ethnicity Indian', 'Ethnicity Latino Hispanic', 'Ethnicity Middle Eastern', 'Ethnicity White', 'Industry', '# Of Entrepreneurs']]], axis=1)\n",
    "\n",
    "\n",
    "#Aggregation  for dimnsionality reduction and interpratibility\n",
    "df_centered['Hand avg velocity Y axis'] = (df_centered['Avg Velocity Right Hand Y'] +  df_centered['Avg Velocity Left Hand Y'] ) / 2\n",
    "df_centered['Hand avg velocity X axis'] = (df_centered['Avg Velocity Right Hand X'] +  df_centered['Avg Velocity Left Hand X'] ) / 2\n",
    "df_centered['Hand avg std velocity'] = (df_centered['STD Velocity Right Hand Y'] + df_centered['STD Velocity Left Hand Y'] + df_centered['STD Velocity Right Hand X'] + df_centered['STD Velocity Left Hand X']) / 4\n",
    "df_centered['Hand velocity magnitude'] = np.sqrt((df_centered['Avg Velocity Right Hand Y'] + df_centered['Avg Velocity Left Hand Y'])**2 + (df_centered['Avg Velocity Right Hand X'] + df_centered['Avg Velocity Left Hand X'])**2)\n",
    "\n",
    "\n",
    "df_centered['Gesture'] = df_centered['Open Palm'] + df_centered['Pointing Up'] + df_centered['Thumb Down'] + df_centered['Thumb Up'] + df_centered['Closed Fist']\n",
    "\n",
    "#df_centered['hand_height'] = (df_centered['right_hand_height'] + df_centered['left_hand_height'] ) / 2\n",
    "#df_centered['hand*height'] = (df_centered['right_hand_height'] * df_centered['left_hand_height'] )\n",
    "\n",
    "\n",
    "#Aggregation for improving performance\n",
    "df_centered['Face Negative'] = df_centered['Face Angry'] + df_centered['Face Disgust'] + df_centered['Face Fear'] + df_centered['Face Sad']\n",
    "df_centered['Face Positive'] = df_centered['Face Happy'] + df_centered['Face Surprise']\n",
    "\n",
    "\n",
    "df_centered['Finance Valence'] = df_centered['Verbal Pos-Finance'] - df_centered['Verbal Neg-Finance']\n",
    "df_centered['Hapiness Valence'] = df_centered['Verbal Pos-Happiness'] - df_centered['Verbal Neg-Happiness']\n",
    "df_centered['Sociability Valence'] = df_centered['Verbal Pos-Sociability'] - df_centered['Verbal Neg-Sociability']\n",
    "df_centered['Morality Valence'] = df_centered['Verbal Pos-Morality'] - df_centered['Verbal Neg-Morality']\n",
    "df_centered['Ability Valence'] = df_centered['Verbal Pos-Ability'] - df_centered['Verbal Neg-Ability']\n",
    "df_centered['Agency Valence'] = df_centered['Verbal Pos-Agency'] - df_centered['Verbal Neg-Agency']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Interaction terms (White becomes a dummy by dropping every other race)\n",
    "\n",
    "df_centered['Jitter * Shimmer'] = df_centered['Jitter'] * df_centered['Shimmer']\n",
    "df_centered['Face Positive * Open Pose'] = df_centered['Face Positive'] * df_centered['Open Pose']\n",
    "df_centered['Jitter * WPM'] = df_centered['Jitter'] * df_centered['WPM']\n",
    "\n",
    "\n",
    "\n",
    "#Interplay between modalities\n",
    "df_centered['Hand velocity magnitude * HNR'] = df_centered['Hand velocity magnitude'] * df_centered['HNR']\n",
    "df_centered['Mean Intensity * Face Happy'] = df_centered['Mean Intensity'] * df_centered['Face Positive']\n",
    "df_centered['Open Pose * HNR'] = df_centered['Open Pose'] * df_centered['HNR']\n",
    "df_centered['Female * Mean Intensity'] = df_centered['Female'] * df_centered['Mean Intensity']\n",
    "df_centered['Face Positive * WPM'] = df_centered['Face Positive'] * df_centered['WPM']\n",
    "df_centered['Face Positive * Sociability Valence'] = df_centered['Face Positive'] * df_centered['Sociability Valence']\n",
    "df_centered['Hapiness Valence * Face Positive'] = df_centered['Hapiness Valence'] * df_centered['Face Positive'] ##!!!!!! IMPORTANT   MEHRHABIan 1971\n",
    "df_centered['Sociability Valence * Open Pose'] = df_centered['Sociability Valence'] * df_centered['Open Pose']\n",
    "df_centered['Finance Valence * Energy'] = df_centered['Finance Valence'] * df_centered['Energy']\n",
    "df_centered['WPM * Sentence Count'] = df_centered['WPM'] * df_centered['Sentence Count']\n",
    "df_centered['Face Negative * Verbal Certainty'] = df_centered['Face Negative'] * df_centered['Verbal Certainty']\n",
    "\n",
    "\n",
    "df_centered['Female * Face Positive'] = df_centered['Female'] * df_centered['Face Positive']\n",
    "df_centered['Female * Gesture'] = df_centered['Female'] * df_centered['Gesture']\n",
    "\n",
    "#Body, Face, Verbal, Vocal, Metadata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Drop columns that we have aggergated or that we dont need\n",
    "df_centered.drop(['ID',\n",
    "\n",
    "        'Avg Velocity Right Hand Y', 'Avg Velocity Left Hand Y', 'Avg Velocity Right Hand X', 'Avg Velocity Left Hand X', 'STD Velocity Right Hand Y', 'STD Velocity Left Hand Y', 'STD Velocity Right Hand X', 'STD Velocity Left Hand X',\n",
    "\n",
    "        'Open Palm', 'Pointing Up', 'Thumb Down', 'Thumb Up','Closed Fist',\n",
    "\n",
    "         'Verbal Pos-Finance', 'Verbal Neg-Finance', 'Verbal Pos-Happiness', 'Verbal Neg-Happiness', 'Verbal Pos-Sociability', 'Verbal Neg-Sociability',\n",
    "         'Verbal Pos-Morality', 'Verbal Neg-Morality', 'Verbal Pos-Ability', 'Verbal Neg-Ability', 'Verbal Pos-Agency', 'Verbal Neg-Agency',\n",
    "\n",
    "        'Ethnicity Asian', 'Ethnicity Black', 'Ethnicity Indian', 'Ethnicity Latino Hispanic', 'Ethnicity Middle Eastern',\n",
    "\n",
    "        'Ask Amount', 'Offered Equity'\n",
    "\n",
    "\n",
    "         #Dropped due to scaling\n",
    "         #'Face Positive',\n",
    "         #'Verbal Pos-Sociability',\n",
    "         #'Jitter',\n",
    "         #'Shimmer',\n",
    "         #'Open Pose',\n",
    "         #'WPM',\n",
    "         #'Mean Pitch',\n",
    "         #'HNR',\n",
    "         #'Hand velocity magnitude',\n",
    "         #'Mean Intensity'\n",
    "\n",
    "\n",
    "\n",
    "        ], axis=1, inplace=True)\n",
    "\n",
    "#Dropped columns due to multicollinearity\n",
    "df_centered.drop([\n",
    "\n",
    "        'Face Neutral', 'Face Angry', 'Face Disgust', 'Face Fear', 'Face Sad', 'Face Happy', 'Face Surprise', #These were not due to multicollinearity but the below are\n",
    "        'Min Intensity',\n",
    "        'Max Intensity',\n",
    "          'Max Pitch',\n",
    "        'Min Pitch'\n",
    "        ], axis=1, inplace=True)\n",
    "\n",
    "rows = df_centered[df_centered.isna().any(axis=1)]\n",
    "print(rows)\n",
    "\n",
    "X = df_centered.drop(columns=['Deal'])\n",
    "y = df_centered['Deal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(X, output_csv_path):\n",
    "    # Add a constant to the independent variables\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Calculate VIF for each feature\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = X.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif_data['Above 10'] = vif_data['VIF'] > 10\n",
    "\n",
    "    # Save the VIF values to a CSV file\n",
    "    vif_data.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    # Display the VIF values in a table format\n",
    "    print(\"\\nVariance Inflation Factor (VIF) Table:\")\n",
    "    print(vif_data)\n",
    "\n",
    "    # Drop the constant column for further analysis if needed\n",
    "    X = X.drop(columns='const')\n",
    "    return X\n",
    "\n",
    "# Example usage\n",
    "data_path = 'path/to/your/data.csv'\n",
    "output_csv_path = 'path/to/output_vif_values.csv'\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming 'X' is your dataframe with independent variables\n",
    "X = data.drop(columns=['target_variable'])  # Replace 'target_variable' with your dependent variable\n",
    "\n",
    "# Calculate VIF\n",
    "X = calculate_vif(X, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_correlation_with_deal(df, target_column, output_figure_path=None):\n",
    "    # Calculate the correlation matrix and sort the correlations with the target column\n",
    "    correlation_matrix = df.corr()[target_column].drop(labels=[target_column]).sort_values(ascending=False)\n",
    "\n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=correlation_matrix.index, y=correlation_matrix.values, palette=\"coolwarm\", edgecolor='k')\n",
    "    plt.title('Correlation with DEAL (Y/N)')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Correlation with DEAL (Y/N)')\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show or save the plot\n",
    "    if output_figure_path:\n",
    "        plt.savefig(output_figure_path)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "data_path = 'path/to/your/data.csv'\n",
    "output_figure_path = 'path/to/output_figure.png'  # Optional, set to None if you don't want to save the plot\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming 'Deal' is your target column\n",
    "target_column = 'Deal'\n",
    "\n",
    "# Plot correlation with Deal\n",
    "plot_correlation_with_deal(df, target_column, output_figure_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_custom_correlation_matrix(df, custom_order, output_figure_path=None):\n",
    "    # Compute the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Identify columns without interaction terms\n",
    "    non_interaction_columns = [col for col in correlation_matrix.columns if '*' not in col and ':' not in col]\n",
    "\n",
    "    # Filter the correlation matrix to remove interaction terms and reorder based on custom order\n",
    "    filtered_correlation_matrix = correlation_matrix.loc[non_interaction_columns, non_interaction_columns]\n",
    "\n",
    "    # Verify that all custom order columns are in the non-interaction columns\n",
    "    custom_order_filtered = [col for col in custom_order if col in non_interaction_columns]\n",
    "\n",
    "    # Reorder the filtered correlation matrix based on the custom order\n",
    "    filtered_correlation_matrix = filtered_correlation_matrix.loc[custom_order_filtered, custom_order_filtered]\n",
    "\n",
    "    # Create a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(filtered_correlation_matrix, dtype=bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(20, 16))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(filtered_correlation_matrix, mask=mask, cmap=cmap, annot=True, fmt=\".2f\",\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .8}, vmin=-1, vmax=1, annot_kws={\"size\": 9})\n",
    "\n",
    "    # Set the title\n",
    "    plt.title('Correlation Matrix', fontsize=20)\n",
    "\n",
    "    # Adjust the font size of the x and y axis labels\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "\n",
    "    # Increase the spacing between the heatmap elements\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure (optional)\n",
    "    if output_figure_path:\n",
    "        plt.savefig(output_figure_path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "data_path = 'path/to/your/data.csv'\n",
    "custom_order = [\n",
    "    # Verbal features\n",
    "    'Verbal Uncertainty', 'Verbal Certainty', 'Finance Valence',\n",
    "    'Verbal Pos-Ability', 'Verbal Pos-Agency', 'Word Count', 'Sentence Count', 'Hapiness Valence', 'Sociability Valence', 'Morality Valence', 'Ability Valence', 'Agency Valence',\n",
    "    # Face-related features\n",
    "    'Face Positive', 'Face Negative',\n",
    "    # Body-related features\n",
    "    'Crossing Arms', 'Hand Distance', 'Left Hand Height', 'Right Hand Height', 'Open Pose',\n",
    "    'Hand avg velocity Y axis', 'Hand avg velocity X axis', 'Hand avg std velocity', 'Hand velocity magnitude', 'Gesture',\n",
    "    # Vocal features\n",
    "    'Mean Pitch', 'STD Pitch', 'Mean Intensity', 'STD Intensity', 'Jitter', 'Shimmer', 'HNR', 'Energy', 'WPM',\n",
    "    # Metadata\n",
    "    'Male', 'Female', 'Deal', 'Industry', '# Of Entrepreneurs', 'Valuation Requested', 'Age', \"Ethnicity White\"\n",
    "]\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Plot the custom correlation matrix\n",
    "output_figure_path = 'path/to/output_figure.png'  # Optional, set to None if you don't want to save the plot\n",
    "plot_custom_correlation_matrix(df, custom_order, output_figure_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def train_random_forest_and_plot_importance(X, y, top_n=12, correlation_threshold=0.022):\n",
    "    # Train a Random Forest model\n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "\n",
    "    # Get feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    feature_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10, 12))  # Increase figure size\n",
    "    feature_importances.sort_values().plot(kind='barh', color='skyblue')\n",
    "    plt.title('Feature Importances')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.xticks(rotation=45)  # Rotate x-ticks if necessary\n",
    "    plt.tight_layout()  # Adjust layout to fit everything nicely\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the correlation coefficients between each feature and the dependent variable\n",
    "    correlations = X.corrwith(y)\n",
    "\n",
    "    # Plot the correlations\n",
    "    plt.figure(figsize=(10, 12))  # Increase figure size\n",
    "    correlations.sort_values().plot(kind='barh', color='salmon')\n",
    "    plt.title('Feature Correlations with Target Variable')\n",
    "    plt.xlabel('Correlation')\n",
    "    plt.ylabel('Features')\n",
    "    plt.xticks(rotation=45)  # Rotate x-ticks if necessary\n",
    "    plt.tight_layout()  # Adjust layout to fit everything nicely\n",
    "    plt.show()\n",
    "\n",
    "    # Select features with correlation above a certain threshold\n",
    "    correlation_selected_features = correlations[(abs(correlations) > correlation_threshold)].index\n",
    "\n",
    "    # Select top N features based on feature importance\n",
    "    top_features = feature_importances.nlargest(top_n).index\n",
    "\n",
    "    # Combine the selected features from both methods\n",
    "    selected_features = list(set(correlation_selected_features).union(set(top_features)))\n",
    "\n",
    "    print(f\"Selected features: {selected_features}\")\n",
    "    return selected_features\n",
    "\n",
    "# Example usage\n",
    "data_path = 'path/to/your/data.csv'\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming 'target' is your dependent variable and others are independent variables\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "# Train Random Forest and plot feature importance\n",
    "selected_features = train_random_forest_and_plot_importance(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "X_selected = X[selected_features]\n",
    "# Subset the data to include only selected features\n",
    "X_audio = X_selected[['HNR', 'Mean Intensity', 'STD Pitch', 'Jitter', 'Energy', 'Shimmer', 'Mean Pitch', 'WPM', 'Jitter * Shimmer', 'STD Intensity']]\n",
    "X_verbal = X_selected[['Finance Valence', 'Verbal Uncertainty', 'Agency Valence', 'Word Count', 'Hapiness Valence', 'Ability Valence', 'Sociability Valence', 'Sentence Count']]\n",
    "X_visual = X_selected[['Hand avg velocity X axis', 'Face Negative', 'Hand Distance', 'Right Hand Height', 'Hand avg std velocity', 'Gesture', 'Face Positive']]\n",
    "\n",
    "############################################################ Combined ############################################################\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'xgboost': {\n",
    "        'n_estimators': [50, 150, 250],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.5, 0.7, 0.9]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [25, 50, 100, 200, 300],\n",
    "        'max_depth': [None, 2, 5, 10, 20],\n",
    "        'min_samples_split': [2, 4, 6, 8],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100, 200, 300, 400, 500],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.005, 0.01, 0.1, 0.3],\n",
    "        'subsample': [0.5, 0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'xgboost': xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, tree_method='gpu_hist', predictor='gpu_predictor'),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'gradient_boosting': GradientBoostingClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'Stage', 'Accuracy', 'F1 Score', 'AUC/ROC', 'Precision', 'Recall'])\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return {'Accuracy': accuracy, 'F1 Score': f1, 'AUC/ROC': auc_roc, 'Precision': precision, 'Recall': recall, 'Confusion Matrix': cm}\n",
    "\n",
    "# Evaluate each model before hyperparameter tuning\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} before tuning...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'Before Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} before tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name} with GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=\"accuracy\", cv=cv, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate each best model on the test set after hyperparameter tuning\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} after tuning...\")\n",
    "    metrics = evaluate_model(best_model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'After Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} after tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Display the final results\n",
    "print(results)\n",
    "\n",
    "\n",
    "############################################################ VISUAL ############################################################\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_visual, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a custom scorer for F1-Macro\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'xgboost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'xgboost': xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, tree_method='gpu_hist', predictor='gpu_predictor'),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'gradient_boosting': GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'Stage', 'Accuracy', 'F1 Score', 'AUC/ROC', 'Precision', 'Recall'])\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return {'Accuracy': accuracy, 'F1 Score': f1, 'AUC/ROC': auc_roc, 'Precision': precision, 'Recall': recall, 'Confusion Matrix': cm}\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate each model before hyperparameter tuning\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} before tuning...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'Before Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} before tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name} with GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=f1_macro_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate each best model on the test set after hyperparameter tuning\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} after tuning...\")\n",
    "    metrics = evaluate_model(best_model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'After Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} after tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Display the results\n",
    "print(results)\n",
    "\n",
    "\n",
    "############################################################ VOCAL ############################################################\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_audio, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a custom scorer for F1-Macro\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'xgboost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'xgboost': xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, tree_method='gpu_hist', predictor='gpu_predictor'),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'gradient_boosting': GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'Stage', 'Accuracy', 'F1 Score', 'AUC/ROC', 'Precision', 'Recall'])\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return {'Accuracy': accuracy, 'F1 Score': f1, 'AUC/ROC': auc_roc, 'Precision': precision, 'Recall': recall, 'Confusion Matrix': cm}\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate each model before hyperparameter tuning\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} before tuning...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'Before Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} before tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name} with GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=f1_macro_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate each best model on the test set after hyperparameter tuning\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} after tuning...\")\n",
    "    metrics = evaluate_model(best_model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'After Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} after tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Display the results\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n",
    "############################################################ VERBAL ############################################################\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_verbal, y, test_size=0.2, random_state=50)\n",
    "\n",
    "# Create a custom scorer for F1-Macro\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'xgboost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'xgboost': xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, tree_method='gpu_hist', predictor='gpu_predictor'),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'gradient_boosting': GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'Stage', 'Accuracy', 'F1 Score', 'AUC/ROC', 'Precision', 'Recall'])\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return {'Accuracy': accuracy, 'F1 Score': f1, 'AUC/ROC': auc_roc, 'Precision': precision, 'Recall': recall, 'Confusion Matrix': cm}\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate each model before hyperparameter tuning\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} before tuning...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'Before Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} before tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name} with GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=f1_macro_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate each best model on the test set after hyperparameter tuning\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} after tuning...\")\n",
    "    metrics = evaluate_model(best_model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'After Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} after tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Display the results\n",
    "print(results)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from matplotlib import pyplot\n",
    "from numpy import where\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'xgboost': {\n",
    "        'n_estimators': [50, 150, 250],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.5, 0.7, 0.9]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [25, 50, 100, 200, 300],\n",
    "        'max_depth': [None, 2, 5, 10, 20],\n",
    "        'min_samples_split': [2, 4, 6, 8],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100, 200, 300, 400, 500],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear'],\n",
    "        'class_weight': ['balanced']\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.005, 0.01, 0.1, 0.3],\n",
    "        'subsample': [0.5, 0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'xgboost': xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, tree_method='gpu_hist', predictor='gpu_predictor'),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'gradient_boosting': GradientBoostingClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'Stage', 'Accuracy', 'F1 Score', 'AUC/ROC', 'Precision', 'Recall'])\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return {'Accuracy': accuracy, 'F1 Score': f1, 'AUC/ROC': auc_roc, 'Precision': precision, 'Recall': recall, 'Confusion Matrix': cm}\n",
    "\n",
    "# Evaluate each model before hyperparameter tuning\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} before tuning...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'Before Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} before tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name} with GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=\"accuracy\", cv=cv, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate each best model on the test set after hyperparameter tuning\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} after tuning...\")\n",
    "    metrics = evaluate_model(best_model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'After Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} after tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Display the final results\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_visual, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a custom scorer for F1-Macro\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'xgboost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'xgboost': xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, tree_method='gpu_hist', predictor='gpu_predictor'),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'gradient_boosting': GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'Stage', 'Accuracy', 'F1 Score', 'AUC/ROC', 'Precision', 'Recall'])\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return {'Accuracy': accuracy, 'F1 Score': f1, 'AUC/ROC': auc_roc, 'Precision': precision, 'Recall': recall, 'Confusion Matrix': cm}\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate each model before hyperparameter tuning\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} before tuning...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'Before Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} before tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name} with GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=f1_macro_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate each best model on the test set after hyperparameter tuning\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} after tuning...\")\n",
    "    metrics = evaluate_model(best_model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'After Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} after tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Display the results\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_audio, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a custom scorer for F1-Macro\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'xgboost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'xgboost': xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, tree_method='gpu_hist', predictor='gpu_predictor'),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'gradient_boosting': GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'Stage', 'Accuracy', 'F1 Score', 'AUC/ROC', 'Precision', 'Recall'])\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return {'Accuracy': accuracy, 'F1 Score': f1, 'AUC/ROC': auc_roc, 'Precision': precision, 'Recall': recall, 'Confusion Matrix': cm}\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate each model before hyperparameter tuning\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} before tuning...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'Before Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} before tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name} with GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=f1_macro_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate each best model on the test set after hyperparameter tuning\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} after tuning...\")\n",
    "    metrics = evaluate_model(best_model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'After Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} after tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Display the results\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, accuracy_score, roc_auc_score, confusion_matrix, roc_curve, precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "import random\n",
    "\n",
    "np.random.seed(50)\n",
    "random.seed(50)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_verbal, y, test_size=0.2, random_state=50)\n",
    "\n",
    "# Create a custom scorer for F1-Macro\n",
    "f1_macro_scorer = make_scorer(f1_score, average='macro')\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    'xgboost': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.3],\n",
    "        'subsample': [0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'xgboost': xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, tree_method='gpu_hist', predictor='gpu_predictor'),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'gradient_boosting': GradientBoostingClassifier(),\n",
    "}\n",
    "\n",
    "# Dictionary to store the best models\n",
    "best_models = {}\n",
    "\n",
    "# DataFrame to store the evaluation results\n",
    "results = pd.DataFrame(columns=['Model', 'Stage', 'Accuracy', 'F1 Score', 'AUC/ROC', 'Precision', 'Recall'])\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_roc = roc_auc_score(y_test, y_prob)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    return {'Accuracy': accuracy, 'F1 Score': f1, 'AUC/ROC': auc_roc, 'Precision': precision, 'Recall': recall, 'Confusion Matrix': cm}\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate each model before hyperparameter tuning\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nEvaluating {model_name} before tuning...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'Before Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} before tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Perform grid search for each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name} with GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring=f1_macro_scorer, cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate each best model on the test set after hyperparameter tuning\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"\\nEvaluating {model_name} after tuning...\")\n",
    "    metrics = evaluate_model(best_model, X_test, y_test)\n",
    "    metrics.update({'Model': model_name, 'Stage': 'After Tuning'})\n",
    "    results = pd.concat([results, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    print(f\"Confusion Matrix for {model_name} after tuning:\\n{metrics['Confusion Matrix']}\")\n",
    "\n",
    "# Display the results\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def plot_shap_summary(best_gb, X_train, X_test):\n",
    "    # SHAP values for the best Gradient Boosting model\n",
    "    explainer = shap.Explainer(best_gb, X_train)\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    # Plot summary plot\n",
    "    shap.summary_plot(shap_values, X_test, max_display=X_train.shape[1])\n",
    "\n",
    "# Example usage\n",
    "data_path = 'path/to/your/data.csv'\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Assuming 'target' is your dependent variable and others are independent variables\n",
    "X = data.drop(columns=['target'])\n",
    "y = data['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Gradient Boosting model (XGBoost in this case)\n",
    "best_gb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "best_gb.fit(X_train, y_train)\n",
    "\n",
    "# Plot SHAP summary\n",
    "plot_shap_summary(best_gb, X_train, X_test)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
